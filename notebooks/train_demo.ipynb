{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc90e16",
   "metadata": {},
   "source": [
    "# Feature Store, Experiment Tracking, Auth, Secrets & Observability Demo\n",
    "This notebook demonstrates integration of Feast (feature store), MLflow (experiment tracking), Keycloak (OIDC), Vault (secrets), Prometheus/Grafana (metrics), OpenTelemetry (tracing), and a mock Arize drift log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6309a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set Environment Variables & Configuration Paths\n",
    "import os, json, datetime, socket\n",
    "\n",
    "def getenv(k, default):\n",
    "    v = os.getenv(k, default)\n",
    "    os.environ[k] = v  # ensure set for later libs\n",
    "    return v\n",
    "\n",
    "FEAST_HOME = getenv(\"FEAST_HOME\", os.path.abspath(\"./services/feast\"))\n",
    "FEAST_PROJECT = getenv(\"FEAST_PROJECT\", \"udo\")\n",
    "FEAST_REDIS_HOST = getenv(\"FEAST_REDIS_HOST\", \"redis\")\n",
    "FEAST_REDIS_PORT = int(getenv(\"FEAST_REDIS_PORT\", \"6379\"))\n",
    "MLFLOW_TRACKING_URI = getenv(\"MLFLOW_TRACKING_URI\", \"http://localhost:5000\")\n",
    "KEYCLOAK_URL = getenv(\"KEYCLOAK_URL\", \"http://localhost:8080\")\n",
    "KEYCLOAK_REALM = getenv(\"KEYCLOAK_REALM\", \"master\")\n",
    "KEYCLOAK_CLIENT_ID = getenv(\"KEYCLOAK_CLIENT_ID\", \"admin-cli\")\n",
    "KEYCLOAK_USERNAME = getenv(\"KEYCLOAK_USERNAME\", \"admin\")\n",
    "KEYCLOAK_PASSWORD = getenv(\"KEYCLOAK_PASSWORD\", \"admin\")  # dev only\n",
    "VAULT_ADDR = getenv(\"VAULT_ADDR\", \"http://localhost:8200\")\n",
    "VAULT_TOKEN = getenv(\"VAULT_TOKEN\", \"root\")\n",
    "OTEL_EXPORTER_OTLP_ENDPOINT = getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://localhost:4317\")\n",
    "\n",
    "print(\"Config Summary\", json.dumps({\n",
    "    'FEAST_HOME': FEAST_HOME,\n",
    "    'FEAST_PROJECT': FEAST_PROJECT,\n",
    "    'MLFLOW_TRACKING_URI': MLFLOW_TRACKING_URI,\n",
    "    'KEYCLOAK_URL': KEYCLOAK_URL,\n",
    "    'VAULT_ADDR': VAULT_ADDR,\n",
    "    'OTEL_EXPORTER_OTLP_ENDPOINT': OTEL_EXPORTER_OTLP_ENDPOINT\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install & Import Dependencies (Feast, MLflow, OIDC, Vault, OTel)\n",
    "import sys, subprocess\n",
    "REQS = [\n",
    "    'feast', 'mlflow', 'pandas', 'scikit-learn', 'python-keycloak', 'hvac',\n",
    "    'opentelemetry-api', 'opentelemetry-sdk', 'opentelemetry-instrumentation-fastapi',\n",
    "    'prometheus-client', 'requests'\n",
    "]\n",
    "for pkg in REQS:\n",
    "    try:\n",
    "        __import__(pkg.split('-')[0])\n",
    "    except ImportError:\n",
    "        print(f'Installing {pkg}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "\n",
    "import pandas as pd, mlflow, feast, requests, hvac\n",
    "from keycloak import KeycloakOpenID\n",
    "from feast import FeatureStore\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "print('Versions:', {\n",
    "    'feast': feast.__version__,\n",
    "    'mlflow': mlflow.__version__,\n",
    "    'pandas': pd.__version__\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Feast Entities and Feature Views (DuckDB/Postgres Source)\n",
    "from pathlib import Path\n",
    "feature_repo_dir = Path(FEAST_HOME)\n",
    "feature_repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "(repo_features := feature_repo_dir / 'features.py').write_text(\"\"\"\n",
    "from feast import Entity, FeatureView, Field, FileSource\n",
    "from feast.types import Float32, Int64\n",
    "product = Entity(name='product_id', join_keys=['product_id'])\n",
    "metrics_source = FileSource(path='samples/products_metrics.csv', timestamp_field=None)\n",
    "product_metrics = FeatureView(\n",
    "    name='product_metrics',\n",
    "    entities=[product],\n",
    "    ttl=None,\n",
    "    schema=[\n",
    "        Field(name='roi', dtype=Float32),\n",
    "        Field(name='revenue', dtype=Int64),\n",
    "        Field(name='cost', dtype=Int64),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=metrics_source,\n",
    ")\n",
    "\"\"\")\n",
    "(feature_store_yaml := feature_repo_dir / 'feature_store.yaml').write_text(f\"\"\"\n",
    "project: {FEAST_PROJECT}\n",
    "registry: {feature_repo_dir / 'registry.db'}\n",
    "provider: local\n",
    "online_store:\n",
    "  type: redis\n",
    "  connection_string: {FEAST_REDIS_HOST}:{FEAST_REDIS_PORT}\n",
    "offline_store:\n",
    "  type: file\n",
    "entity_key_serialization_version: 2\n",
    "\"\"\")\n",
    "print('Feature repo files written:', repo_features, feature_store_yaml)\n",
    "\n",
    "# 4. Create / Update Feast Registry & Materialize to Online Store\n",
    "from feast.repo_operations import apply as feast_apply\n",
    "from feast import FeatureStore as FS\n",
    "store = FS(str(feature_repo_dir))\n",
    "try:\n",
    "    feast_apply(repo_path=str(feature_repo_dir))\n",
    "    from datetime import datetime, timedelta\n",
    "    start = datetime.utcnow() - timedelta(days=1)\n",
    "    end = datetime.utcnow()\n",
    "    store.materialize(start, end)\n",
    "    print('Materialization complete')\n",
    "except Exception as e:\n",
    "    print('Feast apply/materialize error:', e)\n",
    "\n",
    "# 5. Quick Feature Retrieval Test from Feast Online Store\n",
    "try:\n",
    "    fs = FS(str(feature_repo_dir))\n",
    "    feats = fs.get_online_features([\"product_metrics:roi\", \"product_metrics:revenue\"], [{\"product_id\": 1}]).to_dict()\n",
    "    print('Retrieved features:', feats)\n",
    "except Exception as e:\n",
    "    print('Online retrieval failed (expected if Redis not running):', e)\n",
    "\n",
    "# 6. Initialize MLflow Client & Set Tracking URIs\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "experiment_name = 'demo_experiment'\n",
    "exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exp:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "print('Experiments:', [e.name for e in client.list_experiments()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Simulated Training Run (Generate Data, Train Model, Log to MLflow)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(500, 3)\n",
    "y = (X[:,0]*0.3 + X[:,1]*0.5 - X[:,2]*0.2 + 0.1 > 0.5).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run() as run:\n",
    "    model.fit(X_train, y_train)\n",
    "    proba = model.predict_proba(X_test)[:,1]\n",
    "    preds = (proba > 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    mlflow.log_params({'max_iter': 200})\n",
    "    mlflow.log_metrics({'auc': float(auc), 'accuracy': float(acc)})\n",
    "    import tempfile, matplotlib.pyplot as plt\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    fig, ax = plt.subplots(); ax.imshow(cm); ax.set_title('Confusion Matrix'); plt.tight_layout()\n",
    "    tmp_png = tempfile.NamedTemporaryFile(suffix='.png', delete=False).name\n",
    "    plt.savefig(tmp_png)\n",
    "    mlflow.log_artifact(tmp_png, artifact_path='plots')\n",
    "    mlflow.sklearn.log_model(model, 'model')\n",
    "    RUN_ID = run.info.run_id\n",
    "print('Logged run id:', RUN_ID)\n",
    "\n",
    "# 8. Log Feature Metadata & Lineage to MLflow Run\n",
    "with mlflow.start_run(run_id=RUN_ID):\n",
    "    feature_meta = {\n",
    "        'project': FEAST_PROJECT,\n",
    "        'features': ['roi','revenue','cost'],\n",
    "        'generated_at': datetime.datetime.utcnow().isoformat()\n",
    "    }\n",
    "    meta_path = 'features_used.json'\n",
    "    with open(meta_path,'w') as f: json.dump(feature_meta,f)\n",
    "    mlflow.log_artifact(meta_path)\n",
    "    mlflow.set_tags({'feast.project': FEAST_PROJECT, 'feast.feature_count': '3'})\n",
    "print('Feature metadata logged')\n",
    "\n",
    "# 9. Register & Load Model from MLflow (Inference Demo)\n",
    "model_name = 'demo_logreg'\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "try:\n",
    "    mv = client.create_registered_model(model_name)\n",
    "except Exception:\n",
    "    pass\n",
    "client.create_model_version(name=model_name, source=f\"{mlflow.get_tracking_uri()}/#/artifacts/{RUN_ID}/model\", run_id=RUN_ID)\n",
    "loaded = mlflow.sklearn.load_model(f'runs:/{RUN_ID}/model')\n",
    "print('Sample preds:', loaded.predict(X_test[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e97f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Programmatic Keycloak Authentication (Obtain Access Token)\n",
    "try:\n",
    "    keycloak_openid = KeycloakOpenID(server_url=f\"{KEYCLOAK_URL}/\", realm_name=KEYCLOAK_REALM, client_id=KEYCLOAK_CLIENT_ID)\n",
    "    token = keycloak_openid.token(KEYCLOAK_USERNAME, KEYCLOAK_PASSWORD)\n",
    "    access_token = token.get('access_token')\n",
    "    print('Token acquired len=', len(access_token) if access_token else None)\n",
    "except Exception as e:\n",
    "    print('Keycloak token retrieval failed (dev mode maybe not running):', e)\n",
    "    access_token = None\n",
    "\n",
    "# 11. Call Protected FastAPI Endpoint with Bearer Token (placeholder secure endpoint)\n",
    "SECURE_URL = os.getenv('FASTAPI_SECURE_ENDPOINT', 'http://localhost:8000/health')\n",
    "if access_token:\n",
    "    resp = requests.get(SECURE_URL, headers={'Authorization': f'Bearer {access_token}'})\n",
    "    print('Secure endpoint status:', resp.status_code, 'body:', resp.text[:120])\n",
    "else:\n",
    "    print('Skipping secure endpoint call (no token).')\n",
    "\n",
    "# 12. Vault Client Setup (Write / Read Secrets)\n",
    "try:\n",
    "    vault_client = hvac.Client(url=VAULT_ADDR, token=VAULT_TOKEN)\n",
    "    assert vault_client.is_authenticated()\n",
    "    vault_client.secrets.kv.v2.create_or_update_secret(path='airbyte', secret={'password': 'airbyte_pw'})\n",
    "    vault_client.secrets.kv.v2.create_or_update_secret(path='db', secret={'user': 'udo', 'password': 'pass'})\n",
    "    airbyte_secret = vault_client.secrets.kv.v2.read_secret_version(path='airbyte')['data']['data']\n",
    "    print('Vault airbyte secret keys:', list(airbyte_secret.keys()))\n",
    "except Exception as e:\n",
    "    print('Vault interaction failed:', e)\n",
    "\n",
    "# 13. Inject Retrieved Secrets into Config Simulation\n",
    "if 'airbyte_secret' in locals():\n",
    "    os.environ['AIRBYTE_PASSWORD'] = airbyte_secret['password']\n",
    "    connector_cfg = {\n",
    "        'source': 'minio-csv',\n",
    "        'destination': 'postgres',\n",
    "        'credentials_ref': 'vault:airbyte/password'\n",
    "    }\n",
    "    print('Connector config sample:', connector_cfg)\n",
    "else:\n",
    "    print('Skipping secret injection simulation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b41c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. OpenTelemetry Tracing & Metrics Initialization\n",
    "from opentelemetry import trace, metrics\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "from opentelemetry.sdk.metrics import MeterProvider\n",
    "from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n",
    "resource = Resource.create({\"service.name\": \"train-demo\"})\n",
    "tracer_provider = TracerProvider(resource=resource)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "meter_provider = MeterProvider(metric_readers=[PeriodicExportingMetricReader(ConsoleMetricExporter())], resource=resource)\n",
    "metrics.set_meter_provider(meter_provider)\n",
    "meter = metrics.get_meter(\"train-demo\")\n",
    "llm_req_counter = meter.create_counter(\"llm_requests_total\")\n",
    "llm_latency_hist = meter.create_histogram(\"llm_latency_ms\")\n",
    "print('OTel tracing & metrics configured')\n",
    "\n",
    "# 15. Prometheus Metrics Endpoint (local ephemeral)\n",
    "from prometheus_client import start_http_server\n",
    "try:\n",
    "    start_http_server(8009)\n",
    "    print('Started local metrics server on :8009')\n",
    "except Exception as e:\n",
    "    print('Metrics server error:', e)\n",
    "\n",
    "# 16. Load and Inspect Grafana Dashboard JSON Definitions\n",
    "import glob, json as _json\n",
    "for dash_path in glob.glob('monitoring/grafana/dashboards/*.json'):\n",
    "    with open(dash_path) as f:\n",
    "        data = _json.load(f)\n",
    "        print('Dashboard:', data.get('title'), 'Panels:', len(data.get('panels', [])))\n",
    "\n",
    "# 17. Mock LLM Inference & Drift Event Logging\n",
    "import math, time as _time\n",
    "prompt = \"Explain product ROI drivers\"\n",
    "response = \"ROI depends on revenue growth and cost efficiency.\"\n",
    "embedding_norm = math.sqrt(sum(ord(c)%5 for c in response))\n",
    "arize_key = os.getenv('ARIZE_API_KEY')\n",
    "if arize_key:\n",
    "    print('Arize key present - would send event (mock).')\n",
    "else:\n",
    "    drift_log = {'prompt': prompt, 'response': response, 'embedding_norm': embedding_norm, 'ts': _time.time()}\n",
    "    with open('drift_log.json','w') as f: _json.dump(drift_log,f)\n",
    "    print('Drift log written drift_log.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Emit Custom Metrics & Traces for LLM Call\n",
    "import random, time\n",
    "with tracer.start_as_current_span('llm_inference_span') as span:\n",
    "    start = time.time()\n",
    "    # simulate latency\n",
    "    simulated_latency = random.uniform(0.05, 0.25)\n",
    "    time.sleep(simulated_latency)\n",
    "    llm_req_counter.add(1, {\"model\": \"demo-llm\"})\n",
    "    llm_latency_hist.record(simulated_latency * 1000, {\"model\": \"demo-llm\"})\n",
    "    span.set_attribute('llm.model', 'demo-llm')\n",
    "    span.set_attribute('llm.latency_ms', simulated_latency * 1000)\n",
    "print(f'Logged metrics & span (latency_ms={simulated_latency*1000:.1f})')\n",
    "\n",
    "# 19. Consolidated Verification Checks\n",
    "results = {}\n",
    "# Feast registry presence\n",
    "results['feast_registry_exists'] = os.path.exists(os.path.join(FEAST_REPO_ROOT,'registry.db'))\n",
    "# MLflow run exists\n",
    "results['mlflow_run_recorded'] = run is not None and run.info.status == 'FINISHED'\n",
    "# Vault secret file persisted\n",
    "results['vault_secret_file'] = os.path.exists('./injected_secret.txt')\n",
    "# Drift log\n",
    "results['drift_log_written'] = os.path.exists('drift_log.json')\n",
    "# Metrics server placeholder\n",
    "results['prom_metrics_port_active'] = True\n",
    "print('Verification Summary:', results)\n",
    "assert all(results.values()), f\"Some verification checks failed: {results}\"\n",
    "print('All verification checks passed.')\n",
    "\n",
    "# 20. Diagnostics & Cleanup Helpers\n",
    "import psutil, shutil\n",
    "print('Disk usage %:', shutil.disk_usage('.').used / shutil.disk_usage('.').total)\n",
    "print('Active processes sample:', [p.pid for p in psutil.process_iter()][:5])\n",
    "print('Notebook diagnostics complete.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
